{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, Ursula Kaczmarek, Benjamin Feder. \n",
    "\n",
    "_source to be updated when notebook added to GitHub_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "----------\n",
    "Basic dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Methods](#Methods)\n",
    "    - [Python Setup](#Python-Setup)\n",
    "    - [Load the Data](#Load-the-Data)\n",
    "        - [Establish a Connection to the Database](#Establish-a-Connection-to-the-Database)\n",
    "        - [Formulate Data Query](#Formulate-Data-Query)\n",
    "        - [Pull Data from the Database](#Pull-Data-from-the-Database)\n",
    "    - [Analysis: Using Python and SQL](#Analysis:-Using-Python-and-SQL)\n",
    "        - [What is in the Database?](#What-is-in-the-Database?)\n",
    "- [Summary Statistics: TANF Spells](#Summary-Statistics:-TANF-Spells)\n",
    "- [Summary Statistics: Wages](#Summary-Statistics:-Wages)\n",
    "- [Joining Employers and Wage Data](#Joining-Employers-and-Wage-Data)\n",
    "- [Summary Statistics: Employers](#Summary-Statistics:-Employers)\n",
    "- [Creating New Measures](#Creating-New-Measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In an ideal world, we have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true, and we have to use our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF and use them to begin our sample case study: \n",
    "\n",
    "**What does the TANF experience look like? Where are TANF recipients finding employment? Is this different from where they are finding stable employment?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. You will explore relevant datasets in the ADRF and discover different ways to analyze your data. This will be done using both SQL and `pandas` in Python. The `sqlalchemy` Python package will give you the opportunity to interact with the database using SQL to pull data into Python. Some additional manipulations will be handled by `pandas` by converting datasets into dataframes.\n",
    "\n",
    "This notebook will provide an introduction and examples for: \n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to create aggregate metrics\n",
    "- How to handle missing values\n",
    "- How to join newly created tables\n",
    "\n",
    "And the questions below will guide the content in this notebook: \n",
    "\n",
    "__What are different measures of the TANF experience? What are different measures of employment for TANF recipients? Can we find the companies that are common employers of TANF recipients?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using the `sqlalchemy` Python package to access tables in our class database server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using `pandas`, which will read tabular data from SQL queries into a `pandas` DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- `isin`\n",
    "- `groupby`\n",
    "- `nunique`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- Select data subsets\n",
    "- Sum over groups\n",
    "- Create new tables\n",
    "- Count distinct values of desired variables\n",
    "- Order data by chosen variables\n",
    "- Join datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Here are some of the most famous Python packages:\n",
    "- `numpy` is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object and a large suite of functions for doing numerical computing. \n",
    "- `pandas` is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with the language) similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click methods in Microsoft Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- `sqlalchemy` is a Python library for interfacing with a PostGreSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method.__\n",
    "\n",
    "__The `help()` function provides information on what you can do with a function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "help(sqlalchemy.create_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and `pandas` in particular - makes it much easier to calculate descriptive statistics of the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "`pandas` provides many ways to load data. It allows the user to read the data from a local csv or Excel file, pull the data from a relational database, or read directly from a URL (when you have internet access). Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use `pandas` to pull data from a relational database. For examples to read data from a CSV file, refer to the `pandas` documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a `pandas` dataframe (more to come) is `pd.read_sql()`. Just like running a SQL query in DBeaver, this function will ask for some information about the database and the query you would like to run. Let's walk through an example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection, we will use the `SQLAlchemy` package and tell it which database we want to connect to, just like in DBeaver. Additional details on creating a connection to the database are provided in the [Databases](01_0_Database_Connections.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameter 1: Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can parameterize Python `string` objects using the built-in `.format()` function. We will use various formulations in the program notebooks (e.g. when building queries). Some examples are:\n",
    "1. Empty brackets (shown above), which simply inserts the variable in the string; when there is more than one set of brackets, Python will insert variables in the order they are listed\n",
    "2. Brackets with formatting can be used to make print statements more readable (e.g. text with formatted number with comma and 1-digit decimal `{:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`)\n",
    "3. Named brackets to use the same variables multiple times in a text block (we use this in more compicated queries like when creating \"labels\" and \"features\" for machine learning models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This part is similar to writing a SQL query in DBeaver. Depending on the data we are interested in, we can use queries to pull in different data. In this example, we will pull in 20 rows of individual TANF spells data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 entries of the TANF data\n",
    "\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM il_dhs.ind_spells\n",
    "LIMIT 20\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Together, the three quotation marks surrounding the query body is called a multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string instead of breaking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query`, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `LIMIT` statement for two reasons. First, `LIMIT` helps users avoid running into memory issues in Python, as the command controls the maximum amount of rows of the dataframe. Second, it will also speed up some queries for the same reason. Generally, when performing an exploratory data analysis using SQL commands, we recommend you use `LIMIT` to look at a small sample of the data rather than wasting time and potentially creating memory issues by looking at the entire dataset. Sometimes, it may also be advantageous to provide robust `WHERE` clauses that will naturally limit the size of the output, such as restricting the resulting dataset to a specific year. For instance, if you were curious how some metric within the demographic data changed by year, you could start by restricting the dataset to just 2012 and then systematically change the year until you had a full sense of the trend (or lack thereof) in the dataset instead of grouping by the year from the start. Also, you can use PostgreSQL's `EXPLAIN` command to dissect the processing order of the query.\n",
    "\n",
    "> Note that `LIMIT` provides a simple way to get a \"sample\" of data. However, using `LIMIT` **does not provide a _random_ sample**; it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have the two parameters (database connection and query), we can pass them to the `pd.read_sql()` function and obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function \n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first five rows of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Using Python and SQL\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To explore the answers to our guiding questions, we will need to combine various tables together. If you are curious as to how we generated these specific permanent tables available in the `ada_tdc_2019` schema, you can refer to the [Table Creation](Permanent_Table_Creation.ipynb) notebook. Before delving into these tables, though, we will first look at the entire database and the TANF individual spells table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As introduced in the [Databases](01_0_Database_Connections.ipynb) notebook, there are a few ways to connect and explore the data in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Schemas, Tables, and Columns in database__\n",
    "\n",
    "Let's find the list of schema names in the database, the list of tables in these schemas and the list of columns in these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See all available schemas:\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata;\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a reminder, in this class you have access to the following schemas: 'public', 'il_des_kcmo', 'il_dhs', 'in_dwd', 'in_fssa', and 'ada_tdc_2019'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign available schemas to variable schemas\n",
    "schemas = \"\"\"\n",
    "'public', 'il_des_kcmo', 'il_dhs', 'in_dwd', 'in_fssa',  'ada_tdc_2019'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm our schemas exist with an updated version of the previous query\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata\n",
    "WHERE schema_name IN ({})\n",
    "'''.format(schemas)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find all tables within desired schemas\n",
    "query = '''\n",
    "SELECT schemaname, tablename\n",
    "FROM pg_tables\n",
    "WHERE schemaname IN ({})\n",
    "'''.format(schemas)\n",
    "\n",
    "tables = pd.read_sql(query, conn)\n",
    "# print tables not in the public schema\n",
    "print(tables.query(\"schemaname != 'public'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the tables in the IL DHS schema:\n",
    "sorted(tables[tables[\"schemaname\"] == 'il_dhs']['tablename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the two ways shown above to subset a `pandas.DataFrame`:\n",
    "1. Use the built-in `.query()` function\n",
    "2. Create an array of `True` and `False` values (done in this line: `tables[\"schemaname\"] == 'il_dhs'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at column names within tables\n",
    "\n",
    "schema = 'il_dhs'\n",
    "tbl = 'ind_spells'\n",
    "\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '{}' AND table_name = '{}'\n",
    "'''.format(schema, tbl)\n",
    "\n",
    "# read and print results\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 100 rows of tanf data to il_tanf\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM il_dhs.ind_spells\n",
    "limit 100;\n",
    "'''\n",
    "il_tanf = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first five rows in il_tanf\n",
    "il_tanf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, take some time to look at the documentation and understand what the different variables refer to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics: TANF Spells\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this section, we'll explore TANF individual spell data by focusing on any spells for primary TANF recipients in Illinois and Indiana that ended in 2014 Q4. A spell is the time in which an individual/household is receiving aid from TANF.\n",
    "\n",
    "Here are some questions we will use as a guide: \n",
    "\n",
    "- For how long had these individuals been receiving benefits? \n",
    "- Can we find the spell length distribution?\n",
    "\n",
    "To find the answers to these questions, we will refer to the permanent table `ada_tdc_2019.q42014_hoh`, which contains social security numbers for every primary TANF recipient whose benefits ended in 2014 Q4 for both Indiana and Illinois, the state they were receiving the benefits in, and the start date of their spell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out ada_tdc_2019.q42014_hoh\n",
    "qry = '''\n",
    "select *\n",
    "from ada_tdc_2019.q42014_hoh\n",
    "limit 10;\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find spell lengths using age() function\n",
    "qry = '''\n",
    "select count(*), round((end_date - start_date)/30,1) as length\n",
    "from ada_tdc_2019.q42014_hoh\n",
    "group by length\n",
    "order by count desc\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(qry, conn)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25th percentile of spell lengths\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(0.25)\n",
    "    WITHIN GROUP (ORDER BY round((end_date - start_date)/30,1)) as percentile_25\n",
    "FROM ada_tdc_2019.q42014_hoh\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the purposes of your final project outputs, you will be asked to report \"fuzzy\" means to maintain data confidentiality. For example, a fuzzy mean could be reporting the average of the 45th and 55th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of percentile values for spell lengths\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "WITHIN GROUP (ORDER BY round((end_date - start_date)/30,1))\n",
    "FROM ada_tdc_2019.q42014_hoh\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we can use `unnest` to make this a bit easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from above with a reference column and column names for each percentile\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY round((end_date - start_date)/30,1))\n",
    "    ) AS months,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.q42014_hoh\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics: Wages\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this section, we'll dive into a subset of the wage data and analyze a bunch of different measures to compare primary TANF recipients whose spells ended in 2014 Q4 to the entire population of employees in Illinois and Indiana. Here, we will use `ada_tdc_2019.all_wages` that contains all Indiana and Illinois employee data for 2015 Q1, in tandem with `ada_tdc_2019.q42014_cohort_wage`, which contains employee data for each primary TANF recipient whose spell ended in 2014 Q4, to compare the two populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some specific questions to better understand our data:\n",
    "- How many total jobs did individuals hold in 2015 Q1? How many individuals were working those jobs?\n",
    "- What did the wage distribution look like in 2015 Q1?\n",
    "- Can we find out how much each individual was making and how many jobs they had in 2015 Q1?\n",
    "- What was the proportion of TANF individuals with a stable job in 2015?\n",
    "- How many TANF individuals do not have available wage data in our cohort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Large tables__ can take a long time to process on shared databases. The 2015 Q1 Illinois and Indiana combined wage data has more than 9.7 million records, so keep this in mind when running complicated queries on this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of jobs in ada_tdc_2019.all_wages in 2015 Q1\n",
    "qry = '''\n",
    "select count(*)\n",
    "from ada_tdc_2019.all_wages\n",
    "where year = 2015 and quarter = 1\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of people working those jobs in 2015 Q1\n",
    "qry = '''\n",
    "select count(distinct(ssn))\n",
    "from ada_tdc_2019.all_wages\n",
    "where year = 2015 and quarter = 1\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find count of total jobs in 2015 Q1 for primary TANF recipients whose spells ended in Q4 2014 for both states\n",
    "\n",
    "qry = '''\n",
    "select count(*)\n",
    "from ada_tdc_2019.q42014_cohort_wage\n",
    "where year = 2015 and quarter = 1\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of people working those jobs\n",
    "qry = '''\n",
    "select count(distinct(ssn))\n",
    "from ada_tdc_2019.q42014_cohort_wage\n",
    "where year = 2015 and quarter = 1\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wages in 2015 Q1 for full Illinois and Indiana employee data\n",
    "query = '''\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.all_wages\n",
    "WHERE year = 2015 and quarter = 1\n",
    "'''\n",
    "# get results\n",
    "df_earnings = pd.read_sql(query, conn)\n",
    "\n",
    "print(df_earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wages for 2014 Q4 TANF recipients in 2015 Q1\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.q42014_cohort_wage\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wage distribution for just Indiana employees in 2015 Q1, state is coded as 18\n",
    "qry = '''\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.all_wages\n",
    "WHERE state = '18' and year = 2015 and quarter = 1\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wage distribution for Indiana TANF in 2015 Q1\n",
    "\n",
    "qry = '''\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.q42014_cohort_wage\n",
    "WHERE year = 2015 AND quarter = 1 and state = 18\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wage distribution for just illinois employees in 2015 Q1, state is coded as 17\n",
    "qry = '''\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.all_wages\n",
    "WHERE state = '17' and year = 2015 and quarter = 1\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wage distribution for Illinois TANF in 2015 Q1\n",
    "\n",
    "qry = '''\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM ada_tdc_2019.q42014_cohort_wage\n",
    "WHERE year = 2015 AND quarter = 1 and state = 17\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected sample of total wages and amount of jobs per person in 2015 Q1\n",
    "# limit is used to speed up query\n",
    "query = '''\n",
    "select sum(wages), count(*) as num_jobs, ssn\n",
    "from ada_tdc_2019.all_wages\n",
    "where year = 2015 and quarter = 1\n",
    "group by ssn\n",
    "order by ssn\n",
    "limit 100;\n",
    "'''\n",
    "\n",
    "df_full = pd.read_sql(query, conn)\n",
    "\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of total wage per quarter and amount of jobs per 2014 Q4 TANF recipient in 2015\n",
    "query = '''\n",
    "select sum(wages), count(*) as num_jobs, ssn\n",
    "from ada_tdc_2019.q42014_cohort_wage\n",
    "where year = 2015 and quarter = 1\n",
    "group by ssn\n",
    "limit 100;\n",
    "'''\n",
    "\n",
    "df_tanf = pd.read_sql(query, conn)\n",
    "\n",
    "df_tanf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways you could define \"stable employment.\" You could look at full year employment, full quarter employment, or employment over a certain wage. \n",
    "\n",
    "Here, we're going to define stable employment as full quarter employment, meaning that an individual had wage data for the same company for three straight quarters. To do so, we're going to select the same information from `job_yr_q` in `ada_tdc_2019.q42014_cohort_wage` three times and then find if each individual experienced stable employment sometime in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of employees with stable employment at least one quarter in 2015 that had were a primary beneficiary of a TANF spell\n",
    "#that ended in Q4 2014\n",
    "qry = '''\n",
    "select count(distinct(a.ssn))\n",
    "from ada_tdc_2019.q42014_cohort_wage a, ada_tdc_2019.q42014_cohort_wage b, ada_tdc_2019.q42014_cohort_wage c\n",
    "where a.ssn = b.ssn and a.uiacct=b.uiacct and a.state = b.state and a.state = c.state and \n",
    "a.ssn = c.ssn and a.uiacct = c.uiacct and a.job_yr_q = (b.job_yr_q - '3 month'::interval)::date and \n",
    "a.job_yr_q = (c.job_yr_q + '3 month'::interval)::date \n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better sense of the wage data and the differences between subsets of TANF and general employees, we're going to move to analyzing data about the employers, so we can find out the types of companies that are and are not hiring TANF recipients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Employers and Wage Data\n",
    "\n",
    "Previously, to compare the larger cohort of employees to primary recipients who stopped receiving TANF benefits in 2014 Q4, we created another table that filtered the wage data for our desired cohort of TANF recipients. Here, though, since we are analyzing employer data, we will compare all employers who had at least one registered employee in the wage dataset with employers of both cohorts. To make sure each of the employers had at least one employee within our desired time period, we will join the employer and wage datasets into `ada_tdc_2019.combined_employers` AND `ada_tdc_2019.tanf_employers`.\n",
    "\n",
    "Since there are so many entries between these two tables, we dropped some columns that were deemed extraneous in regards to our sample case study and guiding questions for this notebook. The tables contain individual employment data with relevant employer statistics for 2015 Q1 for all employers (`ada_tdc_2019.tanf_employers`) and just those employing primary recipients whose TANF spells ended in 2014 Q4 (`ada_tdc_2019.combined_employers`).\n",
    "\n",
    "The code for creating these tables exists in the [Table Creation](Permanent_Table_Creation.ipynb) notebook. First, let's explore what's in these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 1000 rows of tanf employers\n",
    "qry = '''\n",
    "select *\n",
    "from ada_tdc_2019.tanf_employers\n",
    "limit 1000\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(qry, conn)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What naics codes are in our sample of 1000 jobs? How many employers are associated with each one?\n",
    "df['naics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# size distribution\n",
    "df['size'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count amount of primary TANF recipients whose spells ended in Q4 2014 that are in the wage data\n",
    "query =\"\"\"\n",
    "select count(distinct(ssn_hash))\n",
    "from ada_tdc_2019.q42014_hoh \n",
    "where ssn_hash in (select distinct ssn from ada_tdc_2019.q42014_cohort_wage)\n",
    "\"\"\"\n",
    "\n",
    "has_wages = pd.read_sql(query, conn)\n",
    "\n",
    "has_wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we can access the count in 'has_naics'\n",
    "has_wages['count'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage of total TANF recipients have wages? \n",
    "query =\"\"\"\n",
    "SELECT {}./count(distinct(ssn_hash)) AS not_missing\n",
    "FROM ada_tdc_2019.q42014_hoh;\n",
    "\"\"\".format(has_wages['count'][0])\n",
    "\n",
    "# read the query and print the result\n",
    "print('{:.1f}% of TANF individuals have at least one wage record'\\\n",
    ".format(pd.read_sql(query, conn)['not_missing'][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Discuss with your team:** What we should do about these missing values? We will revisit this discussion in the [Imputting Wages](impute_wages_example.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics: Employers\n",
    "\n",
    "In this section, we'll start looking at aggregate statistics on the employers data in 2015 Q1 and how the total employer data compares to that of employers of primary recipients of TANF benefits that ended during 2014 Q4.\n",
    "\n",
    "Here are some questions we will solve in this section:\n",
    "\n",
    "- How old are these employers? How big are they?\n",
    "- What industries are these employers in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some categories available in the Illinois emplyoers dataset that are not in the Indiana one, like age of the company. Here, we will just analyze Illinois employers of the two cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age of all employers in Illinois distribution for 2015 Q1\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY 2015 - setup_date_year)\n",
    "    ) AS years,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE quarter = 1 and year = 2015\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age of TANF employers distribution for 2015 Q1\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY 2015 - setup_date_year)\n",
    "    ) AS years,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE year = 2015 and quarter = 1 and empr_no in (select distinct uiacct from\n",
    "ada_tdc_2019.tanf_employers where state = 17)\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Indiana employers data also does not contain specific monthly counts of employees for each company in every given year and quarter, while the Illinois table does. Luckily, though, we can use a reasonably accurate proxy by summing up the number distinct social security numbers in the Indiana wage file for each employer and match the counts to the Indiana employer data. We could do the same for the Illinois data, but since we were already provided with the monthly counts, we will utilize the maximum of these three counts for ease of simplicity and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find size distribution of all employers\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9, 0.99])\n",
    "        WITHIN GROUP (ORDER BY size)\n",
    "    ) AS total_size,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9, 0.99]) AS percentile_value\n",
    "FROM ada_tdc_2019.all_employers\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find size distribution of tanf employers\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9, .99])\n",
    "        WITHIN GROUP (ORDER BY size)\n",
    "    ) AS total_size_tanf,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9, .99]) AS percentile_value\n",
    "FROM ada_tdc_2019.tanf_employers\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 of industry breakdown for all employers in 2015 Q1\n",
    "query = \"\"\"\n",
    "select naics, count(distinct(uiacct))\n",
    "from ada_tdc_2019.all_employers\n",
    "group by naics\n",
    "order by count(distinct(uiacct)) desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 of industry breakdown for tanf employers in 2015 Q1\n",
    "query = \"\"\"\n",
    "select naics, count(distinct(uiacct))\n",
    "from ada_tdc_2019.tanf_employers\n",
    "group by naics\n",
    "order by count(distinct(uiacct)) desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of total employers in retail or temporary help in 2015 Q1\n",
    "query = '''\n",
    "select count(distinct(uiacct))\n",
    "from ada_tdc_2019.all_employers\n",
    "where naics = '561' or naics = '451' or naics = '452' or naics = '453' or naics = '454'\n",
    "'''\n",
    "\n",
    "retail_temp = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the SQL query\n",
    "query =\"\"\"\n",
    "SELECT {}./count(*) AS not_missing\n",
    "FROM ada_tdc_2019.all_employers\n",
    "where naics is not null\n",
    "\"\"\".format(retail_temp['count'][0])\n",
    "\n",
    "\n",
    "# read the query and print the result\n",
    "print('{:.1f}% of all employers are in retail or temporary help'\\\n",
    ".format(pd.read_sql(query, conn)['not_missing'][0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of tanf employers in retail or temporary help\n",
    "query = '''\n",
    "select count(distinct(uiacct))\n",
    "from ada_tdc_2019.tanf_employers\n",
    "where (naics = '561' or naics = '451' or naics = '452' or naics = '453' or naics = '454')\n",
    "'''\n",
    "\n",
    "retail_temp = pd.read_sql(query, conn)\n",
    "retail_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the SQL query\n",
    "query =\"\"\"\n",
    "SELECT {}./count(*) AS not_missing\n",
    "FROM ada_tdc_2019.tanf_employers\n",
    "where naics is not null\n",
    "\"\"\".format(retail_temp['count'][0])\n",
    "\n",
    "\n",
    "# read the query and print the result\n",
    "print('{:.1f}% of TANF employers are in retail or temporary help'\\\n",
    ".format(pd.read_sql(query, conn)['not_missing'][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating New Measures\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "To wrap up this notebook, you will go through examples of how to create new columns. Creating new columns oftentimes allows you to hone in on whether a variable meets a certain threshold, and is also of great use in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary Examples**\n",
    "\n",
    "As the notebooks progress we will dig into different aspects of the above questions, but for now we will show two examples of using the `df_jobs` dataframe that is subsetted to wage data for Indiana primary TANF recipients whose spells ended in 2014 Q4 to create two new measures:\n",
    "\n",
    "- Find which TANF leavers experienced another measure of employment in 2015 Q1\n",
    "- Find which employees worked in retail or temporary help sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ssn, wages, quarter and year from sample of in_dwd.wage_by_employer\n",
    "query = '''\n",
    "select ssn, year, quarter, uiacct, wages, naics_3_digit\n",
    "from in_dwd.wage_by_employer \n",
    "where year = 2015 and quarter = 1 and ssn in (select distinct(ssn) from ada_tdc_2019.q42014_hoh where fips = 18)\n",
    "limit 500;\n",
    "'''\n",
    "\n",
    "df_jobs = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since July 24, 2009, the minimum wage in Indiana has been \\\\$7.25 per hour. Assuming a 35 hour work week and 12 weeks in a quarter, someone working an entire quarter at minumum wage would earn \\\\$3,045 in the quarter (ignoring taxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 Q1 earnings are over \"full-time minimum wage\" value of $3,045 in 2015 in Illinois\n",
    "df_jobs[df_jobs['wages']>=3045]['ssn'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column emp_1qtr_overMin for if the individual made more than minimum wage in 2015 Q1\n",
    "df_jobs['emp_1qtr_overMin'] = df_jobs['ssn'].isin(df_jobs[df_jobs['wages']>=3045]['ssn'].unique())\n",
    "\n",
    "# Find the percentage of emp_1qtr_overMin with True\n",
    "df_jobs['emp_1qtr_overMin'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm new column creation\n",
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check records for specific ssn value\n",
    "ssn_val = 'INSERT SSN HASH'\n",
    "df_jobs.query(\"ssn == '{}'\".format(ssn_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dummy column retail_or_temp for retail or temporary work employees\n",
    "df_jobs['retail_or_temp'] = df_jobs['naics_3_digit'].apply(lambda x:True if x in\n",
    "                                                        ['451', '452', '453', '454', '561'] else False)\n",
    "df_jobs['retail_or_temp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you create a similar new column for 2014 Q4 TANF Indiana recipients who worked in the restaurant business?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separate example: Replicating the QWI Statistics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [QWI Statistics](../notebooks_additional/02_3_QWI_stats.ipynb) notebook demonstrates another example of feature creation: the Quarterly Workforce Indicators Census framework using IL wage records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
