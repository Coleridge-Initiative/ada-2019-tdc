{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, Ridhima Sodhi, Ursula Kaczmarek. \n",
    "\n",
    "_source to be updated when notebook added to GitHub_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization in Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "\n",
    "- [Python Setup](#Python-Setup)\n",
    "\n",
    "- [Load the Data](#Load-the-Data)\n",
    "\n",
    "- [Visual Data Exploration with Matplotlib](#Visual-data-exploration-with-matplotlib)\n",
    "    - [A Note on Data Sourcing and Labeling](#A-Note-on-Data-Sourcing-and-Labeling)\n",
    "    - [Mapping with matplotlib](#Mapping-with-matplotlib)\n",
    "    \n",
    "    \n",
    "- [Introducing seaborn](#Introducing-seaborn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this module, you will learn to quickly and flexibly make a wide series of visualizations for exploratory data analysis. This module contains a practical introduction to data visualization in Python and covers important rules that any data visualizer should follow.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "* Become familiar with a core base of data visualization tools in Python - specifically `matplotlib` and `seaborn`\n",
    "\n",
    "* Begin exploring what visualizations are going to best reveal various types of patterns in your data\n",
    "\n",
    "* Learn more about our primary datasets data with exploratory analyses and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation in Python\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# numeric operations\n",
    "import numpy as np\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import seaborn as sn\n",
    "\n",
    "# geography package\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "\n",
    "# database connection\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "\n",
    "# see how long queries and cell content take to run\n",
    "import time\n",
    "\n",
    "# so images get plotted in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Here, we build on the TANF experience explored in the [dataset exploration notebook](01_2_Dataset_Exploration.ipynb) through visualization. We also visualize labor market outcomes of TANF members. For analytical purposes, we will focus on young adult members (we define members as the individuals who qualify for and receive cash benefits) who ended an Indiana or Illinois TANF spell in 2014 Q4 and who were also aged 18-30 at that time. \n",
    "\n",
    "Our guiding questions are as follows:\n",
    "- What are the different ways of visualizing the different TANF experiences of the cohort?\n",
    "- What are the different ways of visualizing the labor market outcomes of this cohort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up sqlalchemy engine\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's start by pulling data on our 2014 Q4 ending spell cohort\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "DROP TABLE IF EXISTS cohort_2014;\n",
    "\n",
    "CREATE TEMP TABLE cohort_2014 AS\n",
    "-- Illinois cohort members\n",
    "SELECT DISTINCT ON (m.ssn_hash) ssn_hash AS member_ssn, 17 AS state, '2014-10-1'::date as end_yr_q,  \n",
    "DATE_PART('year', AGE(i.end_date, m.birth_date))::INT AS member_age, \n",
    "\n",
    "    -- calculate length of spell that is ending this quarter \n",
    "    -- epochs are measured in seconds, so we want to divide by the number of seconds in a month \n",
    "    EXTRACT(EPOCH FROM AGE(i.end_date, i.start_date))/(3600.*672) as spell_len_months\n",
    "\n",
    "FROM il_dhs.indcase_spells i, il_dhs.member_relation r, il_dhs.member m\n",
    "WHERE i.recptno = r.recptno AND i.ch_dpa_caseid = r.ch_dpa_caseid \n",
    "AND i.recptno = m.recptno AND i.ch_dpa_caseid = m.ch_dpa_caseid\n",
    "AND i.end_date BETWEEN '2014-10-01'::DATE AND '2014-12-31'::DATE \n",
    "AND i.benefit_type = 'tanf46'\n",
    "AND reltogte = 82\n",
    "AND DATE_PART('year', AGE(i.end_date, m.birth_date))::INT >= 18 \n",
    "    AND DATE_PART('year', AGE(i.end_date, m.birth_date))::INT <= 30\n",
    "\n",
    "-- Indiana cohort members\n",
    "UNION ALL \n",
    "SELECT DISTINCT ON (ssn) ssn AS member_ssn, 18 AS state, '2014-10-1'::date as end_yr_q,\n",
    "DATE_PART('year', AGE(tanf_end_date::DATE, dob::DATE))::INT AS member_age,\n",
    "\n",
    "-- calculate length of spell that is ending this quarter\n",
    "    EXTRACT(EPOCH FROM AGE(tanf_end_date, tanf_start_date))/(3600.*672) as spell_len_months\n",
    "\n",
    "FROM in_fssa.person_month \n",
    "WHERE tanf_end_date::DATE BETWEEN '2014-10-01'::DATE AND '2014-12-31'::DATE\n",
    "AND tanf = 1\n",
    "AND affil = '1'\n",
    "AND DATE_PART('year', AGE(tanf_end_date::DATE, dob::DATE))::INT >= 18 AND\n",
    "       DATE_PART('year', AGE(tanf_end_date::DATE, dob::DATE))::INT <= 30;\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "conn.execute(query)\n",
    "\n",
    "# report how long creating this table took\n",
    "print('query ran in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eyeball the tanf data\n",
    "df_tanf = pd.read_sql('SELECT * FROM cohort_2014', conn)\n",
    "df_tanf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique members are there in the cohort?\n",
    "df_tanf['member_ssn'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to exploring the TANF experience, we are interested in looking at the characteristics of jobs generating wages for our cohort in the year after their spells end. We will look at two aspects of labor market outcomes:\n",
    "- the size of firms employing members of our cohort (as measured by number of employees)\n",
    "- the quarter over quarter percent change in average wages within the industries in which our cohort members find employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull our cohort's multi-state jobs data\n",
    "query = \"\"\"\n",
    "DROP TABLE IF EXISTS cohort_jobs;\n",
    "\n",
    "CREATE TEMP TABLE cohort_jobs AS\n",
    "SELECT w.*, SUBSTRING(e.naics, 1, 2) as naics\n",
    "FROM ada_tdc_2019.q42014_cohort_wage w, ada_tdc_2019.tanf_employers e\n",
    "WHERE w.uiacct = e.uiacct\n",
    "AND w.state = e.state\n",
    "ORDER BY job_yr_q;\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "conn.execute(query)\n",
    "print('query complete in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate average wages within industries, we must group by naics codes and year/quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_sql('select * from cohort_jobs', conn)\n",
    "df_jobs = df_jobs.groupby(['naics', 'job_yr_q'])['wages'].mean().reset_index()\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull our employer firms' geographic characteristics\n",
    "# Indiana does not have geographic data so we will focus on Illinois\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS il_cohort_firms; \n",
    "\n",
    "CREATE TEMP TABLE il_cohort_firms AS\n",
    "SELECT county, empr_no, SUBSTRING(naics, 1, 2) as naics,\n",
    "GREATEST(empl_month1::INT, empl_month2::INT, empl_month3::INT) AS firm_size,\n",
    "CAST(longitude as DOUBLE PRECISION) AS longitude,\n",
    "CAST(latitude as DOUBLE PRECISION) AS latitude\n",
    "FROM il_des_kcmo.il_qcew_employers \n",
    "WHERE empr_no IN (SELECT DISTINCT ON (uiacct) uiacct FROM cohort_jobs)\n",
    "GROUP BY county, empr_no, naics, longitude, latitude, empl_month1, empl_month2, empl_month3;\n",
    "   \n",
    "COMMIT;\n",
    "\"\"\"\n",
    "conn.execute(sql)\n",
    "print('query complete in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual data exploration with matplotlib\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Under the hood, `Pandas` uses `matplotlib` to produce visualizations. `matplotlib` is the most commonly used base visualization package and allows for a high degree of customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple tanf spell length distributions: histograms\n",
    "df_tanf.spell_len_months.plot(kind='hist', bins=50, alpha=0.6, figsize=(15,5))\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('spell length (months)')\n",
    "plt.title('distribution of cohort spell lengths')\n",
    "plt.annotate('Sources: Illinois Department of Human Services, Indiana Family and Social Services Administration ', \n",
    "             xy=(0.75,-0.15), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize spell length for the younger and older members of our cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's label our cohort members as younger or older\n",
    "df_tanf['age_bin'] = pd.cut(df_tanf['member_age'], bins=[18, 25, 30], labels=['younger', 'older'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tanf.loc[df_tanf['age_bin']=='younger'].spell_len_months.plot(kind='hist', bins=50, alpha=0.6, figsize=(15,5))\n",
    "df_tanf.loc[df_tanf['age_bin']=='older'].spell_len_months.plot(kind='hist', bins=50, alpha=0.6, figsize=(15,5))\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('spell length (months)')\n",
    "plt.title('distribution of cohort spell lengths by age group')\n",
    "plt.legend(['younger (<= 25)', 'older (> 25)'], frameon=False)\n",
    "plt.annotate('Sources: Illinois Department of Human Services, Indiana Family and Social Services Administration', \n",
    "             xy=(0.75,-0.15), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spell length distributions are heavily skewed right. What do they look like if we remove outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the length of our cohort's spells by state and visualize the distributions using another effective chart type for presenting parameters of distributions: boxplots. \n",
    "\n",
    "> Note: Keep in mind that because you cannot export individual observations from the ADRF, you should really only use boxplots here to help guide your data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5)) # this makes it easier to shape a long x-axis\n",
    "df_tanf_il = df_tanf.loc[df_tanf['state']==17]\n",
    "df_tanf_in = df_tanf.loc[df_tanf['state']==18]\n",
    "ax.boxplot([df_tanf_il.spell_len_months, df_tanf_in.spell_len_months], vert=False)\n",
    "ax.set(xlabel='spell length', yticklabels=['illinois', 'indiana'],\\\n",
    "       title='distribution of cohort member spell lengths by state')\n",
    "plt.annotate('Sources: Illinois Department of Human Services, Indiana Family and Social Services Administration', \n",
    "             xy=(0.75,-0.15), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 99th percentile removed\n",
    "fig, ax = plt.subplots(figsize=(15, 5)) # this makes it easier to shape a long x-axis\n",
    "ax.boxplot([df_tanf_il[df_tanf_il.spell_len_months < df_tanf_il.spell_len_months.quantile(0.99)].spell_len_months,\\\n",
    "            df_tanf_in[df_tanf_in.spell_len_months < df_tanf_in.spell_len_months.quantile(0.99)].spell_len_months], vert=False)\n",
    "ax.set(xlabel='spell length (months)', yticklabels=['illinois', 'indiana'],\\\n",
    "       title='distribution of cohort member spell lengths by state (< 99th percentile)')\n",
    "plt.annotate('Sources: Illinois Department of Human Services, Indiana Family and Social Services Administration', \n",
    "             xy=(0.75,-0.15), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Data Sourcing and Labeling\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Data sourcing and labeling axes are critical to effectively conveying information through visualization. Although here we are simply referencing the agencies that created the data, it is ideal to provide as direct a path as possible for the viewer to find the sourced data. When this is not possible (e.g. the data is sequestered), directing the viewer to documentation or methodology for the data is a good alternative. Regardless, providing clear labels and sourcing for the underlying data is an **absolute requirement** of any respectable visualization and further builds trust and enables reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping with matplotlib\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn a bit more about the geographic distribution of firms employing members of the cohort.\n",
    "\n",
    "It contains the firms' county locations, point geographies (latitude, longitude coordinates) of the firm locations and the year of their incorporation. We can read these into a regular `pandas` dataframe and then convert to a `geopandas` geodataframe. A geodataframe has a geometry column of formatted geographies that tells `matplotlib` where the points, polygons, lines, etc. belong on the map.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, because `matplotlib` requires a geometric object to know where to put a point on a map, not just the floats that make up latitude and longitude values, we must add that object with `geopandas` and make a geodataframe.\n",
    "\n",
    "In addition to the geometric object, we must specify the kind of map projection we want. There are numerous coordinate  systems to choose from; the [epsg](http://www.epsg.org) 4269 projection is present in another geodataframe we will use a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first read in our data (where we have coordinate values) as a regular pandas dataframe\n",
    "firms = pd.read_sql('select * from il_cohort_firms where firm_size > 0 and latitude is not null', conn).drop_duplicates()\n",
    "\n",
    "# create our geometric object using the `shapely` package\n",
    "geometry = [shapely.geometry.Point(xy) for xy in zip(firms.longitude, firms.latitude)]\n",
    "\n",
    "\n",
    "# convert our dataframe into a geodataframe: be sure to specify both projection and geometry\n",
    "# we can also drop the latitude & longitude columns as they're now redundant\n",
    "geo_jobs = gpd.GeoDataFrame(firms, geometry = geometry).drop(columns = ['latitude', 'longitude'])\n",
    "geo_jobs.crs = {'init':'epsg:4269'}\n",
    "geo_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite a bit of range in our firm sizes, so let's group them into intervals, just like we did above for ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_jobs['size_bin'] = pd.cut(geo_jobs['firm_size'], bins=[1, 500, 1000, geo_jobs['firm_size'].max()],\\\n",
    "                              labels=[1,2,3])\n",
    "        \n",
    "geo_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need one last thing: the contours of Illinois to give context to our points. We can retrieve the polygons that make up counties within Illinois from the `public.tl_2016_us_county` table. We can filter to Illinois by specifying its state FIPS code using a `WHERE` clause in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT statefp, countyfp, geom FROM public.tl_2016_us_county WHERE statefp = '17'\n",
    "\"\"\"\n",
    "il_polygons = gpd.GeoDataFrame.from_postgis(query, conn, geom_col='geom')\n",
    "il_polygons.crs = geo_jobs.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets plot\n",
    "# we don't really need to see the coordinate values on the axes, so we'll suppress them\n",
    "# we need separate plot() calls for the polygons and the points, but they both need to reference the same axes\n",
    "\n",
    "# create a custom color map for our bin sizes\n",
    "reds = plt.get_cmap('Reds') # use a built-in color palette\n",
    "bins = list(set(geo_jobs['size_bin']))\n",
    "cnorm = colors.Normalize(vmin=0, vmax=len(bins))\n",
    "scalarmap = cmx.ScalarMappable(norm=cnorm, cmap=reds)\n",
    "\n",
    "# color by the firm size group\n",
    "groups = geo_jobs.groupby('size_bin')\n",
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "il_polygons.plot(ax=ax, alpha=0.2, color='lightgray', edgecolor='cadetblue')\n",
    "for name, group in groups:\n",
    "    group.plot(ax=ax, marker='.', label=name, color=scalarmap.to_rgba(name))\n",
    "plt.legend(['1-500', '500-1000', '1000+'], frameon=False)\n",
    "plt.title('Locations and sizes of Illinois Firms Employing the TANF Cohort')\n",
    "plt.annotate('Sources: Illinois Department of Human Services, Indiana Family and Social Services Administration', \n",
    "             xy=(0.75,-0.05), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of occlusion with plotted points, especially in Chicago and other population centers. Let's see if mapping average firm age by county as a choropleth is easier to interpret. We have firm age and county labels in our `geo_jobs` geodataframe, and the county geometries in our `il_polygons` geodataframe. Here we demonstrate how to use the `sjoin()` function to perform a spatial join based on spatial relationships.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the 'how' argument to keep all county polygons, even if there aren't firms in it, to keep the map whole\n",
    "# we use the 'op' argument to tell geopandas that our points reside within our county polygons\n",
    "geo_jobs = geo_jobs[['county', 'firm_size', 'geometry']]\n",
    "joined = gpd.sjoin(il_polygons, geo_jobs, how='left')[['geom', 'countyfp', 'firm_size']]\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the mean firm size by county, we have two options:\n",
    "- `geopandas` `dissolve()` function, which aggregates an attribute value and dissolves all the geometries within a group into a single geometric feature (which has a long runtime)\n",
    "- group by an attribue and `merge()` to a geodataframe\n",
    "\n",
    "We will demonstrate the `merge()` process here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average firm size using a regular dataframe\n",
    "grouped = firms.groupby('county')['firm_size'].mean().reset_index()\n",
    "grouped.columns = ['countyfp', 'avg_firm_size']\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to our polygons\n",
    "merged = il_polygons.merge(grouped, on='countyfp')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot a choropleth of average TANF employer firm size by county in Illinois\n",
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "merged.plot(column='avg_firm_size', ax=ax, cmap='Reds', legend=True)\n",
    "plt.title('Average size of Illinois Firms Employing the TANF Cohort')\n",
    "plt.annotate('Source: Illinois Department of Employment Security', \n",
    "             xy=(0.75,-0.05), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing seaborn\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "`seaborn` is a popular visualization package built on top of `matplotlib` which makes some more complicated graphs easier to make.`seaborn`'s heatmap is an informative way to view a continuous variable. Here we will conclude this notebook by visualizing two categorical variables, industry and year/quarter, and one continuous variable, the percent change in average quarterly wages within industries in which our cohort worked in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['pct_change'] = df_jobs.groupby('naics')['wages'].transform(lambda x: x.pct_change())\n",
    "df_jobs.dropna(inplace=True)\n",
    "df_jobs.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's plot a heatmap\n",
    "plt.figure(figsize=(10, 20))\n",
    "sn.set_style('ticks')\n",
    "sn.heatmap(df_jobs.pivot('naics', 'job_yr_q', 'pct_change'), cmap='coolwarm')\\\n",
    ".set_title('percentage change in wages within cohort-employing industries')\n",
    "plt.annotate('Sources: Illinois Department of Human Services, Indiana Family and Social Services Administration', \n",
    "             xy=(0.75,-0.05), xycoords=\"axes fraction\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {
    "height": "272px",
    "width": "241px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "829px",
    "left": "0px",
    "right": "1548px",
    "top": "110px",
    "width": "301px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
