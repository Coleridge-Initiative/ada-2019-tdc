{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Disclosure Review Examples & Exercises_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides you with information on how to prepare research output for disclosure control. It outlines how to prepare different kind of outputs before submitting an export request and gives you an overview of the information needed for disclosure review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "%pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Remarks on Disclosure Review\n",
    "\n",
    "## Files you can export\n",
    "In general, you can export any kind of file format. However, most research results that researchers typically export are tables, graphs, regression output and aggregated data. Thus, we ask you to export one of these types, implying that every result you would like to export needs to be saved in either .csv, .txt or graph format.\n",
    "\n",
    "## Jupyter notebooks are only exported to retrieve code\n",
    "Unfortunately, you can't export results in a jupyter notebook. Doing disclosure reviews on output in jupyter notebooks is too burdensome for us. Jupyter notebooks will only be exported when the output is deleted for the purpose of exporting code. This does not mean that you won't need your jupyter notebooks during the export process. \n",
    "\n",
    "## Documentation of code is important\n",
    "During the export process, we ask you to provide the code for every output you are asking to export. It is important for the ADRF staff to have the code to better understand what you exactly did. Understanding how research results are created is important to understand your research output. Thus, it is important to document every step of your analysis in your jupyter notebook. \n",
    "\n",
    "## General rules to keep in mind\n",
    "A more detailed description of the rules for exporting results can be found on the class website. This is just a quick overview. We recommend that you to go to the class website and read the guidelines before you prepare your files for export. \n",
    "- The disclosure review is based on the underlying observations of your study. Every statistic you want to export should be based on at least 10 individual data points\n",
    "- Document your code so the reviewer can follow your data work. Assessing re-identification risks highly depends on the context. Thus it is important that you provide context info with your anlysis for the reviewer\n",
    "- Save the requested output with the corresponding code in your input and output folder. Make sure the code is executable. The code should exactly produce the output you requested\n",
    "- If you are exporting powerpoint slides that show project results, you have to provide the code which produces the output in the slide\n",
    "- Please export results only when there are final and you need them for your presentation or final project report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclosure Review Walkthrough\n",
    "\n",
    "We will use Illinois Department of Employment Statistics data to construct our statistics we are interested in and prepare them in a way so we can submit the output for disclosure review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory\n",
    "mypath = (os.getcwd())\n",
    "print(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data\n",
    "\n",
    "In this example we will use `ada_tdc_2019.q42014_cohort_wage`, which is a combination of all jobs during a specific subset of quarters that TANF recipients whose spells ended in 2014 Q4 worked in Indiana and Illinois, and look just at employment in 2015 Q1 for Indiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "from ada_tdc_2019.q42014_cohort_wage\n",
    "where quarter = 1 and year = 2015 and state = 18\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to check dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stats of\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add an earnings categorization for \"low\", \"mid\" and \"high\" using a simple function\n",
    "def earn_calc(earn):\n",
    "    if earn < 5000:\n",
    "        return('low')\n",
    "    elif earn < 10000:\n",
    "        return('mid')\n",
    "    else:\n",
    "        return('high')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earn_calc(24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['earn_cat'] = df['wages'].apply(lambda x: earn_calc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have loaded the data that we need to generate some basic statistics about our populations we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some first desccriptives by group\n",
    "grouped = df.groupby('earn_cat')\n",
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistic in this table will be released if the value is based on at least 10 entities (in this example, individuals). We can see that the total number of individuals we observe in each group completely does not satisfy this (see cell count). However, we also report percentiles, and we report the minimum and maximum value. Especially the minimum and maximum value are most likely representing one individual person. \n",
    "\n",
    "Thus, during disclosure review these values will be supressed. Instead, if we changed our cutoffs in the `earn_calc` function, we might be able to get the statistic(s) based off these groups released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add an earnings categorization for \"low\", \"mid\" and \"high\" using a simple function\n",
    "def earn_calc(earn):\n",
    "    if earn < 5000:\n",
    "        return('low')\n",
    "    elif earn < 7000:\n",
    "        return('mid')\n",
    "    else:\n",
    "        return('high')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some first desccriptives by group\n",
    "grouped = df.groupby('earn_cat')\n",
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can export statistics based on these groups. Let's safely export the statistics in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's export the statistics. Ideally we want to have a csv file\n",
    "# We can safe the statistics in a dataframe\n",
    "export1 = grouped.describe()\n",
    "# and then print to csv\n",
    "export1.to_csv('descriptives_by_group_disclosive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Export of Statistics\n",
    "You can save any dataframe as a csv file and export this csv file. The only thing you have to keep in mind is that besides the statistic X you are interested in, you have to include a variable count of X so we can see on how many observations the statistic is based on. This also applies if you aggregate data. For example if you aggregate by benefit type, we need to know how many observations are in each benefit program (because after the aggregation each benefit type will be only one data point). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problematic Output\n",
    "Some subgroups (e.g. for some of the Illinois datasets dealing with race and gender) will result in cell sizes representing less than 10 people. \n",
    "\n",
    "Tables with cells representing less than 10 individuals won't be released. In this case, disclosure review would mean to delete all cells with counts of less than 10. In addition, secondary suppression has to take place. The disclosure reviewer has to delete as many cells as needed to make it impossible to recalculate the suppressed values. \n",
    "\n",
    "### How to do it better\n",
    "Instead of asking for export of a tables like this, you should prepare your tables in advance that all cell sizes are at least represented by a minimum of 10 observations. \n",
    "\n",
    "### Fuzzy quartiles\n",
    "Percentile values themselves cannot be released as they represent a single observation. Below we show an example of calculating \"fuzzy quartiles\", which is a way to get close to percentiles but still pass disclosure review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.describe(percentiles = [.20, .30, .45, .55, .70, .80])\\\n",
    ".loc[:,idx[('wages'), ('20%', '30%', '45%', '55.0%', '70%', '80%')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_quartiles = grouped.describe(percentiles = [.20, .30, .45, .55, .70, .80])\\\n",
    ".loc[:,idx[('wages'), ('20%', '30%', '45%', '55%', '70%', '80%')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fuzzy_quartiles.loc[:,idx[('wages'), ('20%')]] + fuzzy_quartiles.loc[:,idx[('wages'), ('30%')]])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_quartiles['fuzzy_25th'] = (fuzzy_quartiles.loc[:,idx[('wages'), ('20%')]] + fuzzy_quartiles.loc[:,idx[('wages'), ('30%')]])/2\n",
    "fuzzy_quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do that for the 50th (median) and 75th percentiles:\n",
    "fuzzy_quartiles['fuzzy_median'] = (fuzzy_quartiles.loc[:,idx[('wages'), ('45%')]] + \n",
    "                                 fuzzy_quartiles.loc[:,idx[('wages'), ('55%')]])/2\n",
    "fuzzy_quartiles['fuzzy_75th'] = (fuzzy_quartiles.loc[:,idx[('wages'), ('70%')]] + \n",
    "                                 fuzzy_quartiles.loc[:,idx[('wages'), ('80%')]])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_quartiles.loc[:,idx[(\"fuzzy_25th\", 'fuzzy_median', 'fuzzy_75th'), ('','','')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those \"fuzzy_#\" columns are now safe for export!\n",
    "fuzzy_quartiles.loc[:,idx[(\"fuzzy_25th\", 'fuzzy_median', 'fuzzy_75th'), ('','','')]].to_csv('fuzzy_quartiles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Export of Tables\n",
    "For all tables, you need to provide the underlying counts of the statistics presented in the table. Make sure you provide all counts. If you calculate ratios (e.g. employment rates), you need to provide the count of individuals who are employed and the count of the ones who are not. If you are interested in percentages we still need the underlying counts for disclosure review. Please label the table in a way that we can easily understand what you are plotting. Let's use a similar cohort for this analysis, except just limiting the year to 2015 instead of 2015 Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "from ada_tdc_2019.q42014_cohort_wage\n",
    "where year = 2015 and state = 18\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the earning categories function again to create a new variable earn_cat\n",
    "df['earn_cat'] = df['wages'].apply(lambda x: earn_calc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the categories\n",
    "grouped = df.groupby('earn_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wages by category\n",
    "grouped[['wages']].describe(percentiles = [.5, .9, .99, .999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we are interested in plotting parts of the crosstabulation as a graph\n",
    "# First we need to calulate the counts\n",
    "graph = df.groupby(['earn_cat', 'quarter'])['ssn'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we need to add the unstack command here because our dataframe has nested indices. \n",
    "# We need to flatten out the data before plotting the graph\n",
    "print(graph)\n",
    "print(graph.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate the graph\n",
    "mygraph = graph.unstack().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, it is not clearly visible how many observations are in each bar. Thus we either have to provide a corresponding table (as we generated earlier), or we can use the table=True option to add a table of counts to the graph. In addition, we want to make sure that all our axes and legend are labeled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation including underlying values: the option table=True displays the underlying counts\n",
    "mygraph = graph.unstack().plot(kind='bar', table=True, figsize=(7,5), fontsize=7)\n",
    "# Adjust legend and axes\n",
    "mygraph.legend([\"1\", \"2\", \"3\", \"4\"], loc = 1, ncol= 3, fontsize=9)\n",
    "mygraph.set_ylabel(\"Number of Observations\", fontsize=9)\n",
    "# Add table with counts\n",
    "# We don't need an x axis if we display table\n",
    "mygraph.axes.get_xaxis().set_visible(False)\n",
    "# Grab table info\n",
    "table = mygraph.tables[0]\n",
    "# Format table and figure\n",
    "table.set_fontsize(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're good to go from a counts perspective!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can export the graph as pdf\n",
    "# Save plot to file\n",
    "export2 = mygraph.get_figure()\n",
    "export2.set_size_inches(15,10, forward=True)\n",
    "export2.savefig('barchart_jobs_income_category.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Export of Graphs\n",
    "It is important that every point which is plotted in a graph is based on at least 10 observations. Thus scatterplots for example cannot be released. In case you are interested in exporting a histogram, you have to change the bin size to make sure each bin contains at least 10 people. In addition to the graph, you have to provide the ADRF with the underlying table in a .csv or .txt file. This file should have the same name as the graph so the ADRF staff can directly see which files go together. Alternatively, you can include the counts in the graph as shown in the example above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
